{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "\n",
    "This notebook is used to develop an LSTM model for predicting Dow Jones stocks.  \n",
    "\n",
    "We will begin with the Walmart stock data as a beginning test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "\n",
    "First, we load important packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some useful packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Now, we can load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>unadjustedVolume</th>\n",
       "      <th>change</th>\n",
       "      <th>changePercent</th>\n",
       "      <th>vwap</th>\n",
       "      <th>label</th>\n",
       "      <th>changeOverTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-27</td>\n",
       "      <td>64.7650</td>\n",
       "      <td>64.9747</td>\n",
       "      <td>64.5029</td>\n",
       "      <td>64.7825</td>\n",
       "      <td>9105139</td>\n",
       "      <td>9105139</td>\n",
       "      <td>-0.235889</td>\n",
       "      <td>-0.363</td>\n",
       "      <td>64.7739</td>\n",
       "      <td>Jan 27, 14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-28</td>\n",
       "      <td>64.8786</td>\n",
       "      <td>65.8746</td>\n",
       "      <td>64.7388</td>\n",
       "      <td>65.2368</td>\n",
       "      <td>6035231</td>\n",
       "      <td>6035231</td>\n",
       "      <td>0.454305</td>\n",
       "      <td>0.701</td>\n",
       "      <td>65.3045</td>\n",
       "      <td>Jan 28, 14</td>\n",
       "      <td>0.007013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>65.7785</td>\n",
       "      <td>65.8484</td>\n",
       "      <td>64.7126</td>\n",
       "      <td>64.7388</td>\n",
       "      <td>8440854</td>\n",
       "      <td>8440854</td>\n",
       "      <td>-0.497990</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>61.0517</td>\n",
       "      <td>Jan 29, 14</td>\n",
       "      <td>-0.000675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>65.1232</td>\n",
       "      <td>65.6037</td>\n",
       "      <td>64.9660</td>\n",
       "      <td>65.3067</td>\n",
       "      <td>6742046</td>\n",
       "      <td>6742046</td>\n",
       "      <td>0.567883</td>\n",
       "      <td>0.877</td>\n",
       "      <td>65.2975</td>\n",
       "      <td>Jan 30, 14</td>\n",
       "      <td>0.008092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>64.5816</td>\n",
       "      <td>65.6911</td>\n",
       "      <td>64.3369</td>\n",
       "      <td>65.2455</td>\n",
       "      <td>10665285</td>\n",
       "      <td>10665285</td>\n",
       "      <td>-0.061155</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>65.3223</td>\n",
       "      <td>Jan 31, 14</td>\n",
       "      <td>0.007147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     open     high      low    close    volume  unadjustedVolume  \\\n",
       "0  2014-01-27  64.7650  64.9747  64.5029  64.7825   9105139           9105139   \n",
       "1  2014-01-28  64.8786  65.8746  64.7388  65.2368   6035231           6035231   \n",
       "2  2014-01-29  65.7785  65.8484  64.7126  64.7388   8440854           8440854   \n",
       "3  2014-01-30  65.1232  65.6037  64.9660  65.3067   6742046           6742046   \n",
       "4  2014-01-31  64.5816  65.6911  64.3369  65.2455  10665285          10665285   \n",
       "\n",
       "     change  changePercent     vwap       label  changeOverTime  \n",
       "0 -0.235889         -0.363  64.7739  Jan 27, 14        0.000000  \n",
       "1  0.454305          0.701  65.3045  Jan 28, 14        0.007013  \n",
       "2 -0.497990         -0.763  61.0517  Jan 29, 14       -0.000675  \n",
       "3  0.567883          0.877  65.2975  Jan 30, 14        0.008092  \n",
       "4 -0.061155         -0.094  65.3223  Jan 31, 14        0.007147  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load Walmart Stock Data\n",
    "filepath = os.path.join('..', 'Resources', 'WMT.csv')\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get rid of columns we do not need and set the index as the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-27</td>\n",
       "      <td>64.7650</td>\n",
       "      <td>64.9747</td>\n",
       "      <td>64.5029</td>\n",
       "      <td>64.7825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-28</td>\n",
       "      <td>64.8786</td>\n",
       "      <td>65.8746</td>\n",
       "      <td>64.7388</td>\n",
       "      <td>65.2368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>65.7785</td>\n",
       "      <td>65.8484</td>\n",
       "      <td>64.7126</td>\n",
       "      <td>64.7388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-30</td>\n",
       "      <td>65.1232</td>\n",
       "      <td>65.6037</td>\n",
       "      <td>64.9660</td>\n",
       "      <td>65.3067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>64.5816</td>\n",
       "      <td>65.6911</td>\n",
       "      <td>64.3369</td>\n",
       "      <td>65.2455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     open     high      low    close\n",
       "0  2014-01-27  64.7650  64.9747  64.5029  64.7825\n",
       "1  2014-01-28  64.8786  65.8746  64.7388  65.2368\n",
       "2  2014-01-29  65.7785  65.8484  64.7126  64.7388\n",
       "3  2014-01-30  65.1232  65.6037  64.9660  65.3067\n",
       "4  2014-01-31  64.5816  65.6911  64.3369  65.2455"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop unnessecary columns\n",
    "df.drop(['volume', 'unadjustedVolume', 'change', 'changePercent', 'vwap', 'label', 'changeOverTime'], 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-27</th>\n",
       "      <td>64.7650</td>\n",
       "      <td>64.9747</td>\n",
       "      <td>64.5029</td>\n",
       "      <td>64.7825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-28</th>\n",
       "      <td>64.8786</td>\n",
       "      <td>65.8746</td>\n",
       "      <td>64.7388</td>\n",
       "      <td>65.2368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-29</th>\n",
       "      <td>65.7785</td>\n",
       "      <td>65.8484</td>\n",
       "      <td>64.7126</td>\n",
       "      <td>64.7388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-30</th>\n",
       "      <td>65.1232</td>\n",
       "      <td>65.6037</td>\n",
       "      <td>64.9660</td>\n",
       "      <td>65.3067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-31</th>\n",
       "      <td>64.5816</td>\n",
       "      <td>65.6911</td>\n",
       "      <td>64.3369</td>\n",
       "      <td>65.2455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               open     high      low    close\n",
       "date                                          \n",
       "2014-01-27  64.7650  64.9747  64.5029  64.7825\n",
       "2014-01-28  64.8786  65.8746  64.7388  65.2368\n",
       "2014-01-29  65.7785  65.8484  64.7126  64.7388\n",
       "2014-01-30  65.1232  65.6037  64.9660  65.3067\n",
       "2014-01-31  64.5816  65.6911  64.3369  65.2455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set index\n",
    "df.set_index('date', inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10d294a20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot close price\n",
    "df.plot(y='close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test Split Data\n",
    "\n",
    "We need to create a train/test split.  To do so, we will assume we feed sequences of some length and predict at some point in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1259, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save data as a matrix\n",
    "data = df.values\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save sequence length and time in the future\n",
    "#we will start with 30 days and 5 days in the future (about 1 month and 1 week)\n",
    "seq_length = 30\n",
    "fut_point = 5\n",
    "features = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1224, 30, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get X data (30 day sequences)\n",
    "X = []\n",
    "#get all sequences up to (sequence length + future point) days out of last point (can then predict last point)\n",
    "for index in range(len(data) - seq_length - fut_point):\n",
    "    X.append(data[index: index + seq_length])\n",
    "#get X as a numpy array\n",
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1224,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get Y data (close price for all days except first (sequence length + future point) days)\n",
    "y = data[(seq_length + fut_point):, -1]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 30, 4)\n",
      "(184, 30, 4)\n",
      "(1040,)\n",
      "(184,)\n"
     ]
    }
   ],
   "source": [
    "#train/test split of 0.85/0.15\n",
    "train_split = 0.85\n",
    "last_row = int(train_split * X.shape[0])\n",
    "X_train = X[:last_row]\n",
    "X_test = X[last_row:]\n",
    "y_train = y[:last_row]\n",
    "y_test = y[last_row:]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data\n",
    "We scale the data using the MinMaxScaler fit for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate scalers\n",
    "X_scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "y_scaler = MinMaxScaler(feature_range = (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape data so it can be fit\n",
    "X_train_reshaped = np.reshape(X_train, (-1, 4))\n",
    "X_test_reshaped = np.reshape(X_test, (-1, 4))\n",
    "y_train_reshaped = np.reshape(y_train, (-1, 1))\n",
    "y_test_reshaped = np.reshape(y_test, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(-1, 1))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit scalers\n",
    "X_scaler.fit(X_train_reshaped)\n",
    "y_scaler.fit(y_train_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1040, 30, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform and rescale\n",
    "X_train_scaled = np.reshape(X_scaler.transform(X_train_reshaped), X_train.shape)\n",
    "X_test_scaled = np.reshape(X_scaler.transform(X_test_reshaped), X_test.shape)\n",
    "y_train_scaled = np.reshape(y_scaler.transform(y_train_reshaped), y_train.shape[0])\n",
    "y_test_scaled = np.reshape(y_scaler.transform(y_test_reshaped), y_test.shape[0])\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model\n",
    "Now, we build a basic LSTM network model.\n",
    "\n",
    "We build several LSTM layers together, adding Dropout layers and a few dense layers to summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PythonData/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import layers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers. import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 30, 256)           267264    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 800,833\n",
      "Trainable params: 800,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create an LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "#add first LSTM layer and dropout layer\n",
    "model.add(LSTM(256, return_sequences = True, input_shape = (seq_length, features)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#add second LSTM layer and dropout layer\n",
    "model.add(LSTM(256, return_sequences = False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#add an reLU layer\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "#add a final layer\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "#compile model\n",
    "model.compile(loss = 'mse', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 884 samples, validate on 156 samples\n",
      "Epoch 1/300\n",
      "884/884 [==============================] - 3s 4ms/step - loss: 0.0414 - acc: 0.0011 - val_loss: 0.0694 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0087 - acc: 0.0011 - val_loss: 0.0662 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0066 - acc: 0.0011 - val_loss: 0.0693 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0056 - acc: 0.0011 - val_loss: 0.0780 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0054 - acc: 0.0011 - val_loss: 0.0854 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0055 - acc: 0.0011 - val_loss: 0.0931 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0053 - acc: 0.0011 - val_loss: 0.1094 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0051 - acc: 0.0011 - val_loss: 0.1195 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0055 - acc: 0.0011 - val_loss: 0.1158 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0051 - acc: 0.0011 - val_loss: 0.1141 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0048 - acc: 0.0011 - val_loss: 0.1161 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0053 - acc: 0.0011 - val_loss: 0.1122 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0049 - acc: 0.0011 - val_loss: 0.1066 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0049 - acc: 0.0011 - val_loss: 0.1162 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0048 - acc: 0.0011 - val_loss: 0.1154 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.0999 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0049 - acc: 0.0011 - val_loss: 0.0940 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0050 - acc: 0.0011 - val_loss: 0.1020 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0049 - acc: 0.0011 - val_loss: 0.1029 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.1071 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.1123 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0049 - acc: 0.0011 - val_loss: 0.1140 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.1006 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.0928 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.0950 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0045 - acc: 0.0011 - val_loss: 0.0900 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0046 - acc: 0.0011 - val_loss: 0.0953 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0049 - acc: 0.0011 - val_loss: 0.0980 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0045 - acc: 0.0011 - val_loss: 0.0978 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.1079 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0048 - acc: 0.0011 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0045 - acc: 0.0011 - val_loss: 0.1355 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0045 - acc: 0.0011 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.1265 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.1256 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.1220 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0049 - acc: 0.0011 - val_loss: 0.1119 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0054 - acc: 0.0011 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0046 - acc: 0.0011 - val_loss: 0.1492 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.1585 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.1621 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0046 - acc: 0.0011 - val_loss: 0.1450 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.1257 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.1462 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0048 - acc: 0.0011 - val_loss: 0.1629 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.1759 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.1639 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.1575 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.1472 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.1521 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.1490 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.1671 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0046 - acc: 0.0011 - val_loss: 0.1970 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0049 - acc: 0.0011 - val_loss: 0.2040 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0052 - acc: 0.0011 - val_loss: 0.1993 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.2106 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0046 - acc: 0.0011 - val_loss: 0.2743 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.3165 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.3284 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.3033 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.2847 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.2807 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.2751 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.2864 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.2895 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.3121 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0047 - acc: 0.0011 - val_loss: 0.3393 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0045 - acc: 0.0011 - val_loss: 0.2972 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0044 - acc: 0.0011 - val_loss: 0.2856 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.3016 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.3178 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0045 - acc: 0.0011 - val_loss: 0.3454 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.2965 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.2698 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.2687 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.2809 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.2962 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.2996 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.2953 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.2885 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.3403 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.3739 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.3225 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.2832 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.3172 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0043 - acc: 0.0011 - val_loss: 0.2520 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.2757 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.2118 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.2121 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.1783 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.1917 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.2280 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.2183 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.1880 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.2428 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.2015 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.1567 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.1670 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.1473 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.1514 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.1533 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.1034 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.0941 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.1179 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.0753 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.0574 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.0535 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.0623 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.1103 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.0355 - val_acc: 0.0064\n",
      "Epoch 112/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0045 - acc: 0.0011 - val_loss: 0.0431 - val_acc: 0.0064\n",
      "Epoch 113/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.0606 - val_acc: 0.0064\n",
      "Epoch 114/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.0801 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.0768 - val_acc: 0.0064\n",
      "Epoch 116/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0634 - val_acc: 0.0064\n",
      "Epoch 117/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.0628 - val_acc: 0.0064\n",
      "Epoch 118/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0038 - acc: 0.0011 - val_loss: 0.0984 - val_acc: 0.0064\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.1052 - val_acc: 0.0064\n",
      "Epoch 120/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.0901 - val_acc: 0.0064\n",
      "Epoch 121/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0713 - val_acc: 0.0064\n",
      "Epoch 122/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0038 - acc: 0.0011 - val_loss: 0.0649 - val_acc: 0.0064\n",
      "Epoch 123/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0667 - val_acc: 0.0064\n",
      "Epoch 124/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.1345 - val_acc: 0.0064\n",
      "Epoch 125/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.0939 - val_acc: 0.0064\n",
      "Epoch 126/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0042 - acc: 0.0011 - val_loss: 0.0463 - val_acc: 0.0064\n",
      "Epoch 127/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0038 - acc: 0.0011 - val_loss: 0.1705 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0038 - acc: 0.0011 - val_loss: 0.0935 - val_acc: 0.0064\n",
      "Epoch 129/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0436 - val_acc: 0.0064\n",
      "Epoch 130/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0312 - val_acc: 0.0064\n",
      "Epoch 131/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.0641 - val_acc: 0.0064\n",
      "Epoch 132/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.0432 - val_acc: 0.0064\n",
      "Epoch 133/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0409 - val_acc: 0.0064\n",
      "Epoch 134/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0509 - val_acc: 0.0064\n",
      "Epoch 135/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0011 - val_loss: 0.0606 - val_acc: 0.0064\n",
      "Epoch 136/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.0406 - val_acc: 0.0064\n",
      "Epoch 137/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0561 - val_acc: 0.0064\n",
      "Epoch 138/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.0409 - val_acc: 0.0064\n",
      "Epoch 139/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0334 - val_acc: 0.0064\n",
      "Epoch 140/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0547 - val_acc: 0.0064\n",
      "Epoch 141/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0628 - val_acc: 0.0064\n",
      "Epoch 142/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.0806 - val_acc: 0.0064\n",
      "Epoch 143/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0704 - val_acc: 0.0064\n",
      "Epoch 144/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.0823 - val_acc: 0.0064\n",
      "Epoch 145/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.1804 - val_acc: 0.0064\n",
      "Epoch 146/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0011 - val_loss: 0.0395 - val_acc: 0.0064\n",
      "Epoch 147/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.2283 - val_acc: 0.0064\n",
      "Epoch 148/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.1353 - val_acc: 0.0064\n",
      "Epoch 149/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.1566 - val_acc: 0.0064\n",
      "Epoch 150/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.2052 - val_acc: 0.0064\n",
      "Epoch 151/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.3024 - val_acc: 0.0064\n",
      "Epoch 152/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0383 - val_acc: 0.0064\n",
      "Epoch 153/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0950 - val_acc: 0.0064\n",
      "Epoch 154/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0038 - acc: 0.0011 - val_loss: 0.0827 - val_acc: 0.0064\n",
      "Epoch 155/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0370 - val_acc: 0.0064\n",
      "Epoch 156/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0336 - val_acc: 0.0064\n",
      "Epoch 157/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0354 - val_acc: 0.0064\n",
      "Epoch 158/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.0782 - val_acc: 0.0064\n",
      "Epoch 159/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0442 - val_acc: 0.0064\n",
      "Epoch 160/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.1895 - val_acc: 0.0064\n",
      "Epoch 161/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.0630 - val_acc: 0.0064\n",
      "Epoch 162/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0357 - val_acc: 0.0064\n",
      "Epoch 163/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0536 - val_acc: 0.0064\n",
      "Epoch 164/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.1653 - val_acc: 0.0064\n",
      "Epoch 165/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0597 - val_acc: 0.0064\n",
      "Epoch 166/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.1304 - val_acc: 0.0064\n",
      "Epoch 167/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0832 - val_acc: 0.0064\n",
      "Epoch 168/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.1042 - val_acc: 0.0064\n",
      "Epoch 169/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.1650 - val_acc: 0.0064\n",
      "Epoch 170/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0638 - val_acc: 0.0064\n",
      "Epoch 171/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.1198 - val_acc: 0.0064\n",
      "Epoch 172/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0542 - val_acc: 0.0064\n",
      "Epoch 173/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.1021 - val_acc: 0.0064\n",
      "Epoch 174/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.1391 - val_acc: 0.0064\n",
      "Epoch 175/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.1255 - val_acc: 0.0064\n",
      "Epoch 176/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0848 - val_acc: 0.0064\n",
      "Epoch 177/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.1816 - val_acc: 0.0064\n",
      "Epoch 178/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0038 - acc: 0.0011 - val_loss: 0.1134 - val_acc: 0.0064\n",
      "Epoch 179/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0473 - val_acc: 0.0064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0740 - val_acc: 0.0064\n",
      "Epoch 181/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.0413 - val_acc: 0.0064\n",
      "Epoch 182/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0570 - val_acc: 0.0064\n",
      "Epoch 183/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.0992 - val_acc: 0.0064\n",
      "Epoch 184/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0040 - acc: 0.0011 - val_loss: 0.0476 - val_acc: 0.0064\n",
      "Epoch 185/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0413 - val_acc: 0.0064\n",
      "Epoch 186/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0707 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0419 - val_acc: 0.0064\n",
      "Epoch 188/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0554 - val_acc: 0.0064\n",
      "Epoch 189/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0721 - val_acc: 0.0064\n",
      "Epoch 190/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0011 - val_loss: 0.1026 - val_acc: 0.0064\n",
      "Epoch 191/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0775 - val_acc: 0.0064\n",
      "Epoch 192/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.1041 - val_acc: 0.0064\n",
      "Epoch 193/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0559 - val_acc: 0.0064\n",
      "Epoch 194/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0602 - val_acc: 0.0064\n",
      "Epoch 195/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0489 - val_acc: 0.0064\n",
      "Epoch 196/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0621 - val_acc: 0.0064\n",
      "Epoch 197/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0729 - val_acc: 0.0064\n",
      "Epoch 198/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0778 - val_acc: 0.0064\n",
      "Epoch 199/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1664 - val_acc: 0.0064\n",
      "Epoch 200/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0843 - val_acc: 0.0064\n",
      "Epoch 201/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0866 - val_acc: 0.0064\n",
      "Epoch 202/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0037 - acc: 0.0011 - val_loss: 0.0883 - val_acc: 0.0064\n",
      "Epoch 203/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0626 - val_acc: 0.0064\n",
      "Epoch 204/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0463 - val_acc: 0.0064\n",
      "Epoch 205/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0764 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "884/884 [==============================] - 2s 3ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0379 - val_acc: 0.0064\n",
      "Epoch 207/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0415 - val_acc: 0.0064\n",
      "Epoch 208/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0432 - val_acc: 0.0064\n",
      "Epoch 209/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0334 - val_acc: 0.0064\n",
      "Epoch 210/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0732 - val_acc: 0.0064\n",
      "Epoch 211/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0852 - val_acc: 0.0064\n",
      "Epoch 212/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0463 - val_acc: 0.0064\n",
      "Epoch 213/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0485 - val_acc: 0.0064\n",
      "Epoch 214/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0433 - val_acc: 0.0064\n",
      "Epoch 215/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0807 - val_acc: 0.0064\n",
      "Epoch 216/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0496 - val_acc: 0.0064\n",
      "Epoch 217/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0942 - val_acc: 0.0064\n",
      "Epoch 218/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0843 - val_acc: 0.0064\n",
      "Epoch 219/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0603 - val_acc: 0.0064\n",
      "Epoch 220/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.0484 - val_acc: 0.0064\n",
      "Epoch 221/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0519 - val_acc: 0.0064\n",
      "Epoch 222/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0543 - val_acc: 0.0064\n",
      "Epoch 223/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0487 - val_acc: 0.0064\n",
      "Epoch 224/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0500 - val_acc: 0.0064\n",
      "Epoch 225/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.0450 - val_acc: 0.0064\n",
      "Epoch 226/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0623 - val_acc: 0.0064\n",
      "Epoch 227/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.0505 - val_acc: 0.0064\n",
      "Epoch 228/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.0587 - val_acc: 0.0064\n",
      "Epoch 229/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0529 - val_acc: 0.0064\n",
      "Epoch 230/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0549 - val_acc: 0.0064\n",
      "Epoch 231/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0662 - val_acc: 0.0064\n",
      "Epoch 232/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0518 - val_acc: 0.0064\n",
      "Epoch 233/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.0730 - val_acc: 0.0064\n",
      "Epoch 234/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.1350 - val_acc: 0.0064\n",
      "Epoch 235/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0811 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.0999 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0553 - val_acc: 0.0064\n",
      "Epoch 238/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0502 - val_acc: 0.0064\n",
      "Epoch 239/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0797 - val_acc: 0.0064\n",
      "Epoch 240/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0432 - val_acc: 0.0064\n",
      "Epoch 241/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0682 - val_acc: 0.0064\n",
      "Epoch 242/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.0368 - val_acc: 0.0064\n",
      "Epoch 243/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0029 - acc: 0.0011 - val_loss: 0.0744 - val_acc: 0.0064\n",
      "Epoch 244/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.0589 - val_acc: 0.0064\n",
      "Epoch 245/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0035 - acc: 0.0011 - val_loss: 0.1166 - val_acc: 0.0064\n",
      "Epoch 246/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0598 - val_acc: 0.0064\n",
      "Epoch 247/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.0441 - val_acc: 0.0064\n",
      "Epoch 248/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.1042 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0377 - val_acc: 0.0064\n",
      "Epoch 250/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.0634 - val_acc: 0.0064\n",
      "Epoch 251/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.1550 - val_acc: 0.0064\n",
      "Epoch 252/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0029 - acc: 0.0011 - val_loss: 0.0718 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0690 - val_acc: 0.0064\n",
      "Epoch 254/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.0620 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.1049 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.1376 - val_acc: 0.0064\n",
      "Epoch 257/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.1346 - val_acc: 0.0064\n",
      "Epoch 258/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1534 - val_acc: 0.0064\n",
      "Epoch 259/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.1505 - val_acc: 0.0064\n",
      "Epoch 260/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1656 - val_acc: 0.0064\n",
      "Epoch 261/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1490 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1421 - val_acc: 0.0064\n",
      "Epoch 263/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.0939 - val_acc: 0.0064\n",
      "Epoch 264/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0746 - val_acc: 0.0064\n",
      "Epoch 265/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.0855 - val_acc: 0.0064\n",
      "Epoch 266/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.2634 - val_acc: 0.0064\n",
      "Epoch 267/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0029 - acc: 0.0011 - val_loss: 0.1298 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.1803 - val_acc: 0.0064\n",
      "Epoch 269/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1030 - val_acc: 0.0064\n",
      "Epoch 270/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1715 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0029 - acc: 0.0011 - val_loss: 0.1227 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.0615 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.1012 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.1021 - val_acc: 0.0064\n",
      "Epoch 275/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1737 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.2188 - val_acc: 0.0064\n",
      "Epoch 277/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0011 - val_loss: 0.3478 - val_acc: 0.0064\n",
      "Epoch 278/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0029 - acc: 0.0011 - val_loss: 0.1879 - val_acc: 0.0064\n",
      "Epoch 279/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.5154 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.7714 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.2814 - val_acc: 0.0064\n",
      "Epoch 282/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.4096 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.5145 - val_acc: 0.0064\n",
      "Epoch 284/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 1.1115 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0029 - acc: 0.0011 - val_loss: 1.1323 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 1.1300 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.8911 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 1.0515 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0034 - acc: 0.0011 - val_loss: 0.1678 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.2283 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.1635 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.0934 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1000 - val_acc: 0.0064\n",
      "Epoch 294/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.1122 - val_acc: 0.0064\n",
      "Epoch 295/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.1219 - val_acc: 0.0064\n",
      "Epoch 296/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.1243 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0030 - acc: 0.0011 - val_loss: 0.0890 - val_acc: 0.0064\n",
      "Epoch 298/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.0917 - val_acc: 0.0064\n",
      "Epoch 299/300\n",
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0031 - acc: 0.0011 - val_loss: 0.0680 - val_acc: 0.0064\n",
      "Epoch 300/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "884/884 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0011 - val_loss: 0.1371 - val_acc: 0.0064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2861e6d8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "model.fit(X_train_scaled, y_train_scaled, epochs = 300, batch_size = 64, validation_split = 0.15, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
